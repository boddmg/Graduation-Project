@article{Rybok,
abstract = {Object information is an important cue to discriminate between activities that draw part of their meaning from context. Most of current work either ignores this information or relies on specific object detectors. However, such object detectors require a significant amount of training data and complicate the transfer of the action recognition framework to novel domains with different objects and object-action relationships. Motivated by recent advances in saliency detection, we propose to use proto-objects to detect object candidate regions in videos without any need of prior knowledge. Our experimental evaluation on three publicly available data sets shows that the integration of proto-objects and simple motion features substantially improves recognition performance, outperforming the stateof-the-art.},
author = {Rybok, Lukas and Schauerte, Boris and Al-Halah, Ziad and Stiefelhagen, Rainer},
doi = {10.1109/WACV.2014.6836041},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/【Important Stuff, Everywhere!】 Activity Recognition with Salient Proto-Objects as Context.pdf:pdf},
isbn = {9781479949854},
journal = {2014 IEEE Winter Conference on Applications of Computer Vision, WACV 2014},
pages = {646--651},
title = {{"Important stuff, everywhere!" Activity recognition with salient proto-objects as context}},
year = {2014}
}
@inproceedings{RefWorks:doc:55619df3e4b030e10feadeb2,
abstract = {This paper presents a novel approach for combining optical flow into enhanced 3D motion vector fields for human action recognition. Our approach detects motion of the actors by computing optical flow in video data captured by a multi-view camera setup with an arbitrary number of views. Optical flow is estimated in each view and extended to 3D using 3D reconstructions of the actors and pixel-to-vertex correspondences. The resulting 3D optical flow for each view is combined into a 3D motion vector field by taking the significance of local motion and its reliability into account. 3D Motion Context (3D-MC) and Harmonic Motion Context (HMC) are used to represent the extracted 3D motion vector fields efficiently and in a view-invariant manner, while considering difference in anthropometry of the actors and their movement style variations. The resulting 3D-MC and HMC descriptors are classified into a set of human actions using normalized correlation, taking into account the performing speed variations of different actors. We compare the performance of the 3D-MC and HMC descriptors, and show promising experimental results for the publicly available i3DPost Multi View Human Action Dataset.},
author = {Holte, Michael B. and Moeslund, Thomas B. and Nikolaidis, Nikos and Pitas, Ioannis},
booktitle = {2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission},
doi = {10.1109/3DIMPVT.2011.50},
isbn = {978-0-7695-4369-7},
keywords = {3D motion description,3D optical flow,human action recognition,multi-view},
pages = {342--349},
publisher = {IEEE},
title = {{3D Human Action Recognition for Multi-view Camera Systems}},
url = {http://seu.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwpV1LS8NAEF6qJ0FQsWLVwv6BtLtJNsketVotIgQpgqcw-wKhlNLH\_3dmm9bHxYOQwy57SLKbmf0yO983jGXpQCS\_fIKT2unKyQAyD6C18YU1MpQmCEjLSGMbvavHuhjflXWH7aosUr2TmIrmB9SMJ\_vYXw3BrBqYzaKsICzb4hlKKzRf-n\_HXZm-8GLyvI-3kBw},
year = {2011}
}
@article{Shan2014,
author = {Shan, Junjie and Akella, Srinivas},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/3D Human Action Segmentation and Recognition using Pose Kinetic Energy.pdf:pdf},
isbn = {9781479969685},
title = {{3D Human Action Segmentation and Recognition using Pose Kinetic Energy}},
year = {2014}
}
@article{Jones2014,
author = {Jones, Simon and Shao, Ling},
doi = {10.1109/CVPR.2014.110},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/Jones\_A\_Multigraph\_Representation\_2014\_CVPR\_paper.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Cvpr},
title = {{A Multigraph Representation for Improved Unsupervised/Semi-supervised Learning of Human Actions}},
url = {http://lshao.staff.shef.ac.uk/pub/Multigraph\_CVPR2014.pdf$\backslash$npapers3://publication/uuid/55B8A94B-B4C4-4D2A-B32D-7AF7D5112209},
year = {2014}
}
@article{Faria2014,
author = {Faria, Diego R. and Premebida, Cristiano and Nunes, Urbano},
doi = {10.1109/ROMAN.2014.6926340},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/A Probalistic Approach for Human Everyday Activities Recognition using Body Motion from RGB-D Images.pdf:pdf},
isbn = {9781479967650},
journal = {IEEE International Symposium on Robot and Human Interactive Communication (Ro-Man)},
title = {{A Probabilistic Approach for Human Everyday Activities Recognition using Body Motion from RGB-D Images}},
year = {2014}
}
@article{Chen2013,
abstract = {Analysis of human behaviour through visual information has been a highly active research topic in the computer vision community. This was previously achieved via images from a conventional camera, however recently depth sensors have made a new type of data available. This survey starts by explaining the advantages of depth imagery, then describes the new sensors that are available to obtain it. In particular, the Microsoft Kinect has made high-resolution real-time depth cheaply available. The main published research on the use of depth imagery for analysing human activity is reviewed. Much of the existing work focuses on body part detection and pose estimation. A growing research area addresses the recognition of human actions. The publicly available datasets that include depth imagery are listed, as are the software libraries that can acquire it from a sensor. This survey concludes by summarising the current state of work on this topic, and pointing out promising future research directions. For both researchers and practitioners who are familiar with this topic and those who are new to this field, the review will aid in the selection, and development, of algorithms using depth data. ?? 2013 Elsevier B.V. All rights reserved.},
author = {Chen, Lulu and Wei, Hong and Ferryman, James},
doi = {10.1016/j.patrec.2013.02.006},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/A survey of human motion analysis using depth imagery.pdf:pdf},
isbn = {0167-8655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {3D body model,Depth sensor,Human action recognition,Human pose estimation,Range data,Survey},
number = {15},
pages = {1995--2006},
publisher = {Elsevier B.V.},
title = {{A survey of human motion analysis using depth imagery}},
url = {http://dx.doi.org/10.1016/j.patrec.2013.02.006},
volume = {34},
year = {2013}
}
@article{Chaquet2013,
abstract = {Vision-based human action and activity recognition has an increasing importance among the computer vision community with applications to visual surveillance, video retrieval and human-computer interaction. In recent years, more and more datasets dedicated to human action and activity recognition have been created. The use of these datasets allows us to compare different recognition systems with the same input data. The survey introduced in this paper tries to cover the lack of a complete description of the most important public datasets for video-based human activity and action recognition and to guide researchers in the election of the most suitable dataset for benchmarking their algorithms. © 2013 Elsevier Inc. All rights reserved.},
author = {Chaquet, Jose M. and Carmona, Enrique J. and Fern\'{a}ndez-Caballero, Antonio},
doi = {10.1016/j.cviu.2013.01.013},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/A survey of video datasets for human action and activity recognition.pdf:pdf},
isbn = {10773142},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Database,Dataset,Human action recognition,Human activity recognition,Review,Survey},
number = {6},
pages = {633--659},
publisher = {Elsevier Inc.},
title = {{A survey of video datasets for human action and activity recognition}},
url = {http://dx.doi.org/10.1016/j.cviu.2013.01.013},
volume = {117},
year = {2013}
}
@article{Poppe2010,
abstract = {Vision-based human action recognition is the process of labeling image sequences with action labels. Robust solutions to this problem have applications in domains such as visual surveillance, video retrieval and human-computer interaction. The task is challenging due to variations in motion performance, recording settings and inter-personal differences. In this survey, we explicitly address these challenges. We provide a detailed overview of current advances in the field. Image representations and the subsequent classification process are discussed separately to focus on the novelties of recent research. Moreover, we discuss limitations of the state of the art and outline promising directions of research. ?? 2009 Elsevier B.V. All rights reserved.},
author = {Poppe, Ronald},
doi = {10.1016/j.imavis.2009.11.014},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/A survey on vision-based human action recognition.pdf:pdf},
isbn = {0262-8856},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Action detection,Human action recognition,Motion analysis},
number = {6},
pages = {976--990},
publisher = {Elsevier B.V.},
title = {{A survey on vision-based human action recognition}},
url = {http://dx.doi.org/10.1016/j.imavis.2009.11.014},
volume = {28},
year = {2010}
}
@inproceedings{RefWorks:doc:55619e03e4b030e10feadeb3,
abstract = {In this paper, we address the problem of learning compact, view-independent, realistic 3D models of human actions recorded with multiple cameras, for the purpose of recognizing those same actions from a single or few cameras, without prior knowledge about the relative orientations between the cameras and the subjects. To this aim, we propose a new framework where we model actions using three dimensional occupancy grids, built from multiple viewpoints, in an exemplar-based HMM. The novelty is, that a 3D reconstruction is not required during the recognition phase, instead learned 3D exemplars are used to produce 2D image information that is compared to the observations. Parameters that describe image projections are added as latent variables in the recognition process. In addition, the temporal Markov dependency applied to view parameters allows them to evolve during recognition as with a smoothly moving camera. The effectiveness of the framework is demonstrated with experiments on real datasets and with challenging recognition scenarios.},
author = {Weinland, Daniel and Boyer, Edmond and Ronfard, Remi},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2007.4408849},
isbn = {978-1-4244-1631-8},
issn = {1550-5499},
pages = {1--7},
publisher = {IEEE},
title = {{Action recognition from arbitrary views using 3D exemplars}},
url = {http://seu.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwpV1bT8IwGG3QJxMTbxhRSfoHBrv06oMJIqhvxBASn5Z2a9UIhDBI-Pn26zYEX3zwrU2Xbe2W9nyX8x2EkrgTBr\_2BMsZVUY5OCdlJhWVhuQJ0w6-G0W5Bv9b\_40-jdjwgY8aqFZZBL0Tn4pmOtD0kX3XL7pKF6maTn1ZQbWsxDNAPFkQIPNFcUhLLtdOBI-L-t\_},
year = {2007}
}
@article{Koppula2013,
abstract = {We represent each possible future using an anticipatory temporal conditional random field (ATCRF) that models the rich spatial-temporal relations through object affordances. We then consider each ATCRF as a particle and represent the distribution over the potential futures using a set of particles.},
author = {Koppula, Hema Swetha and Saxena, Ashutosh},
doi = {10.1109/IROS.2013.6696634},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Anticipating Human Activities for Reactive Robotic Response.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {2071},
title = {{Anticipating human activities for reactive robotic response}},
year = {2013}
}
@book{Bellman1957,
abstract = {An introduction to the mathematical theory of multistage decision processes, this text takes a "functional equation" approach to the discovery of optimum policies. Written by a leading developer of such policies, it presents a series of methods, uniqueness and existence theorems, and examples for solving the relevant equations. The text examines existence and uniqueness theorems, the optimal inventory equation, bottleneck problems in multistage production processes, a new formalism in the calculus of variation, strategies behind multistage games, and Markovian decision processes. Each chapter concludes with a problem set that Eric V. Denardo of Yale University, in his informative new introduction, calls "a rich lode of applications and research topics." 1957 edition. 37 figures.},
archivePrefix = {arXiv},
arxivId = {1004.2027v1},
author = {Bellman, R E},
booktitle = {Annals of Operations Research},
doi = {10.1007/BF02188548},
eprint = {1004.2027v1},
isbn = {069107951X},
issn = {02545330},
number = {1},
pages = {343--395},
pmid = {16590080},
title = {{Chapter 6 Dynamic programming algorithms}},
url = {http://www.springerlink.com/index/10.1007/BF02188548},
volume = {11},
year = {1987}
}
@article{Hinton2006,
abstract = {We show how to use complementary priors to eliminate the explaining- away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associa- tive memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive ver- sion of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribu- tion of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning al- gorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/A fast learning algorithm for deep belief nets.pdf:pdf},
journal = {Neural Computation},
pages = {1527--1554},
title = {{Communicated by Yann Le Cun A Fast Learning Algorithm for Deep Belief Nets 500 units 500 units}},
volume = {18},
year = {2006}
}
@article{Zhang2014,
author = {Zhang, Yu},
doi = {10.1109/CVPR.2014.121},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/Zhang\_Compact\_Representation\_for\_2014\_CVPR\_paper.pdf:pdf},
isbn = {978-1-4799-5118-5},
pages = {907--914},
title = {{Compact Representation for Image Classification : To Choose or to Compress ?}},
year = {2014}
}
@article{Schmidhuber2014,
abstract = {In recent years, deep neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.7828v1},
author = {Schmidhuber, J},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {arXiv:1404.7828v1},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/Deep learning in neural networks An overview.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {arXiv preprint arXiv:1404.7828},
pages = {1--66},
publisher = {Elsevier Ltd},
title = {{Deep Learning in Neural Networks: An Overview}},
url = {http://arxiv.org/abs/1404.7828},
volume = {61},
year = {2014}
}
@article{Lecun2013,
abstract = {ICML2013 Tutorial},
author = {Lecun, Yann},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/deeplearning.pdf:pdf},
journal = {ICML2013 Tutorial},
title = {{Deep Learning Tutorial}},
year = {2013}
}
@article{Arel2010,
author = {Arel, Itamar and Rose, Derek C and Karnowski, Thomas P},
file = {:Users/boddmg/work/ml/Graduation-Project/resource/paper/Deep machine learning - a new frontier in artificial intelligence research.pdf:pdf},
journal = {IEEE Computational Inteligence Magazine},
keywords = {deep-learning,review},
number = {4},
pages = {13--18},
title = {{Deep Machine Learning — A New Frontier in Artificial Intelligence Research}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5605630},
volume = {5},
year = {2010}
}
@article{Yang2014,
abstract = {In this paper, we propose an effective method to recognize human actions using 3D skeleton joints recovered from 3D depth data of RGBD cameras. We design a new action feature descriptor for action recognition based on differences of skeleton joints, i.e., EigenJoints which combine action information including static posture, motion property, and overall dynamics. Accumulated Motion Energy (AME) is then proposed to perform informative frame selection, which is able to remove noisy frames and reduce computational cost. We employ non-parametric Na??ve-Bayes-Nearest-Neighbor (NBNN) to classify multiple actions. The experimental results on several challenging datasets demonstrate that our approach outperforms the state-of-the-art methods. In addition, we investigate how many frames are necessary for our method to perform classification in the scenario of online action recognition. We observe that the first 30-40\% frames are sufficient to achieve comparable results to that using the entire video sequences on the MSR Action3D dataset. ?? 2013 Elsevier Inc. All rights reserved.},
author = {Yang, Xiaodong and Tian, Yingli},
doi = {10.1016/j.jvcir.2013.03.001},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Effective 3D action recognition using EigenJoints.pdf:pdf},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
keywords = {3D action feature representation,Accumulated motion energy,Action recognition,Depth data,Informative frame selection,Na??ve-Bayes-Nearest-Neighbor,RGBD camera,Skeleton joints},
number = {1},
pages = {2--11},
publisher = {Elsevier Inc.},
title = {{Effective 3D action recognition using EigenJoints}},
url = {http://dx.doi.org/10.1016/j.jvcir.2013.03.001},
volume = {25},
year = {2014}
}
@article{Ranzato2007,
abstract = {We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces "stroke detectors" when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps.},
author = {Ranzato, Marc Aurelio and Poultney, Christopher and Chopra, Sumit and Lecun, Yann},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/Efficient Learning of Sparse Representations.pdf:pdf},
isbn = {9780262195683},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1137--1134},
title = {{Efficient Learning of Sparse Representations with an Energy-Based Model}},
volume = {19},
year = {2007}
}
@article{Zhu2014,
abstract = {Human action recognition has lots of real-world applications, such as natural user interface, virtual reality, intelligent surveillance, and gaming. However, it is still a very challenging problem. In action recognition using the visible light videos, the spatiotemporal interest point (STIP) based features are widely used with good performance. Recently, with the advance of depth imaging technology, a new modality has appeared for human action recognition. It is important to assess the performance and usefulness of the STIP features for action analysis on the new modality of 3D depth map. In this paper, we evaluate the spatiotemporal interest point (STIP) based features for depth-based action recognition. Different interest point detectors and descriptors are combined to form various STIP features. The bag-of-words representation and the SVM classifiers are used for action learning. Our comprehensive evaluation is conducted on four challenging 3D depth databases. Further, we use two schemes to refine the STIP features, one is to detect the interest points in RGB videos and apply to the aligned depth sequences, and the other is to use the human skeleton to remove irrelevant interest points. These refinements can help us have a deeper understanding of the STIP features on 3D depth data. Finally, we investigate a fusion of the best STIP features with the prevalent skeleton features, to present a complementary use of the STIP features for action recognition on 3D data. The fusion approach gives significantly higher accuracies than many state-of-the-art results. © 2014 Elsevier B.V.},
author = {Zhu, Yu and Chen, Wenbin and Guo, Guodong},
doi = {10.1016/j.imavis.2014.04.005},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Evaluating spatiotemporal interest point features for depth-based action recognition .pdf:pdf},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Action recognition,Descriptors,Detectors,Evaluation,Feature fusion,RGB-D sensor,STIP feature refinement,STIP features,Spatiotemporal interest point (STIP)},
number = {8},
pages = {453--464},
publisher = {Elsevier B.V.},
title = {{Evaluating spatiotemporal interest point features for depth-based action recognition}},
url = {http://dx.doi.org/10.1016/j.imavis.2014.04.005},
volume = {32},
year = {2014}
}
@article{Piyathilaka2013,
abstract = {Ability to recognize human activities will enhance the capabilities of a robot that interacts with humans. However automatic detection of human activities could be challenging due to the individual nature of the activities. In this paper, we present human activity detection model that uses only 3-D skeleton features generated from an RGB-D sensor (Microsoft Kinect TM). To infer the human activities, we implemented Gaussian Mixture Modal (GMM) based Hidden Markov Model(HMM). GM outputs of the HMM were effectively able to capture multimodel nature of 3D positions of each skeleton joint. We test our model in a publicly available data-set that consists of twelve different daily activities performed by four different people.The proposed model recorded recognition recall accuracy of 84\% with previously seen people and 78\% with previously unseen people.},
author = {Piyathilaka, Lasitha and Kodagoda, Sarath},
doi = {10.1109/ICIEA.2013.6566433},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Gaussian Mixture Based HMM for Human Daily Activity Recognition Using 3D Skeleton Features.pdf:pdf},
isbn = {9781467363211},
journal = {Proceedings of the 2013 IEEE 8th Conference on Industrial Electronics and Applications, ICIEA 2013},
pages = {567--572},
title = {{Gaussian mixture based HMM for human daily activity recognition using 3D skeleton features}},
year = {2013}
}
@inproceedings{LeCun1998,
abstract = {A long and detailed paper on convolutional nets, graph transformer$\backslash$nnetworks, and discriminative training methods for sequence labeling.$\backslash$nWe show how to build systems that integrate segmentation, feature$\backslash$nextraction, classification, contextual post-processing, and language$\backslash$nmodeling into one single learning machine trained end-to-end. Applications$\backslash$nto handwriting recognition and face detection are described.},
author = {LeCun, Y and Bottou, L and Bengio, Y and Haffner, P},
booktitle = {Proceedings of the IEEE},
file = {:Users/boddmg/Library/Application Support/Mendeley Desktop/Downloaded/LeCun et al. - 1998 - Gradient Based Learning Applied to Document Recognition(2).pdf:pdf},
number = {11},
pages = {2278--2324},
title = {{Gradient Based Learning Applied to Document Recognition}},
volume = {86},
year = {1998}
}
@inproceedings{Dalal2005,
abstract = {We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Dalal, Navneet and Triggs, Bill},
booktitle = {Proceedings - 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, CVPR 2005},
doi = {10.1109/CVPR.2005.177},
eprint = {9411012},
isbn = {0769523722},
issn = {1063-6919},
keywords = {human-detection,local-feature,object-detection},
pages = {886--893},
pmid = {9230594},
primaryClass = {chao-dyn},
title = {{Histograms of oriented gradients for human detection}},
url = {citeulike-article-id:3047126$\backslash$nhttp://dx.doi.org/10.1109/CVPR.2005.177},
volume = {I},
year = {2005}
}
@article{Chaudhry2009,
abstract = {System theoretic approaches to action recognition model the dynamics of a scene with linear dynamical systems (LDSs) and perform classification using metrics on the space of LDSs, e.g. Binet-Cauchy kernels. However, such approaches are only applicable to time series data living in a Euclidean space, e.g. joint trajectories extracted from motion capture data or feature point trajectories extracted from video. Much of the success of recent object recognition techniques relies on the use of more complex feature descriptors, such as SIFT descriptors or HOG descriptors, which are essentially histograms. Since histograms live in a non-Euclidean space, we can no longer model their temporal evolution with LDSs, nor can we classify them using a metric for LDSs. In this paper, we propose to represent each frame of a video using a histogram of oriented optical flow (HOOF) and to recognize human actions by classifying HOOF time-series. For this purpose, we propose a generalization of the Binet-Cauchy kernels to nonlinear dynamical systems (NLDS) whose output lives in a non-Euclidean space, e.g. the space of histograms. This can be achieved by using kernels defined on the original non-Euclidean space, leading to a well-defined metric for NLDSs. We use these kernels for the classification of actions in video sequences using (HOOF) as the output of the NLDS. We evaluate our approach to recognition of human actions in several scenarios and achieve encouraging results.},
author = {Chaudhry, R. and Ravichandran, a. and Hager, G. and Vidal, R.},
doi = {10.1109/CVPR.2009.5206821},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Histograms of Oriented Optical Flow and Binet-Cauchy Kernels on Nonlinear Dynamical Systems for the Recognition of Human Actions.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
keywords = {*file-import-13-02-06},
pages = {1932--1939},
title = {{Histograms of oriented optical flow and Binet-Cauchy kernels on nonlinear dynamical systems for the recognition of human actions}},
year = {2009}
}
@article{Vemulapalli,
author = {Vemulapalli, Raviteja and Arrate, Felipe and Chellappa, Rama},
doi = {10.1109/CVPR.2014.82},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Vemulapalli\_Human\_Action\_Recognition\_2014\_CVPR\_paper.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
title = {{Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group}},
year = {2014}
}
@article{Tang2014,
author = {Tang, Nick C and Lin, Yen-yu and Weng, Ming-fang},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/HUMAN ACTION RECOGNITION USING ASSOCIATED DEPTH AND SKELETON INFORMATION.pdf:pdf},
isbn = {9781479928934},
pages = {4641--4645},
title = {{HUMAN ACTION RECOGNITION USING ASSOCIATED DEPTH AND SKELETON INFORMATION Ju-Hsuan Hua Institute of Information Science , Academia Sinica , Taiwan Research Center for Information Technology Innovation , Academia Sinica , Taiwan Smart Network System Institu}},
year = {2014}
}
@article{Lertniphonphan2011,
abstract = {Recognizing human actions is a challenging research area due to the complexity and variation of human's appearances and postures, the variation of camera settings, and angles. In this paper, we introduce a motion descriptor based on direction of optical flow for human action recognition. The directional value of a silhouette is divided into small regions. In each region, the normalized direction histogram of optical flow is computed. The motion vector is the values of a histogram in every region respective concatenation. The vectors are smoothed in time domain by moving average to reduce the motion variation and noise. For the training process, the motion vectors of the training set are clustered by K-mean to represent action. The clustered data group the similar posture together and is represented by the cluster centers. The centers are used to compare input frames by computing distance and using K-nearest neighbor to classify action. The experimental results show that K-mean clustering can group the similar pose together. The motion feature can be used to classify action in a low resolution image with a small number of reference vectors.},
author = {Lertniphonphan, Kanokphan and Aramvith, Supavadee and Chalidabhongse, Thanarat H.},
doi = {10.1109/ISCIT.2011.6089701},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Human action recognition using direction histograms of optical flow.pdf:pdf},
isbn = {978-1-4577-1293-7},
journal = {2011 11th International Symposium on Communications \& Information Technologies (ISCIT)},
keywords = {direction histogram,human action,optical flow,recognition},
number = {Iscit},
pages = {574--579},
title = {{Human action recognition using direction histograms of optical flow}},
volume = {c},
year = {2011}
}
@article{Gupta2013,
author = {Gupta, Raj and Chia, Alex Yong-Sang and Rajan, Deepu},
doi = {10.1145/2502081.2502099},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Human Activities Recognition using Depth Images.pdf:pdf},
isbn = {978-1-4503-2404-5},
journal = {Proceedings of the 21st ACM international conference on Multimedia},
keywords = {depth image segmentation,human activity detection,institute for infocomm research,part of this work,singapore,was done while alex,yong-sang chia was with},
pages = {283--292},
title = {{Human activities recognition using depth images}},
url = {http://doi.acm.org/10.1145/2502081.2502099},
year = {2013}
}
@article{Sung2012,
abstract = {Being able to detect and recognize human activities is important for making personal assistant robots useful in performing assistive tasks. The challenge is to develop a system that is low-cost, reliable in unstructured home settings, and also straightforward to use. In this paper, we use a RGBD sensor (Microsoft Kinect) as the input sensor, and present learning algorithms to infer the activities. Our algorithm is based on a hierarchical maximum entropy Markov model (MEMM). It considers a person's activity as composed of a set of sub-activities, and infers the two-layered graph structure using a dynamic programming approach. We test our algorithm on detecting and recognizing twelve different activities performed by four people in different environments, such as a kitchen, a living room, an office, etc., and achieve an average performance of 84.3\% when the person was seen before in the training set (and 64.2\% when the person was not seen before).},
archivePrefix = {arXiv},
arxivId = {1107.0169},
author = {Sung, Jaeyong and Ponce, Colin and Selman, Bart and Saxena, Ashutosh},
doi = {citeulike-article-id:9510173},
eprint = {1107.0169},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Unstructured Human Activity Detection from RGBD Images.pdf:pdf},
isbn = {9781577355328},
title = {{Human Activity Detection from RGBD Images}},
url = {http://arxiv.org/abs/1107.0169},
year = {2011}
}
@article{Aggarwal2014,
abstract = {Human activity recognition has been an important area of computer vision research since the 1980s. Various approaches have been proposed with a great portion of them addressing this issue via conventional cameras. The past decade has witnessed a rapid development of 3D data acquisition techniques. This paper summarizes the major techniques in human activity recognition from 3D data with a focus on techniques that use depth data. Broad categories of algorithms are identified based upon the use of different features. The pros and cons of the algorithms in each category are analyzed and the possible direction of future research is indicated. © 2014 Elsevier B.V. All rights reserved.},
author = {Aggarwal, J. K. and Xia, Lu},
doi = {10.1016/j.patrec.2014.04.011},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Human activity recognition from 3D data-A review.pdf:pdf},
isbn = {9781479956869},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {3D data,Computer vision,Depth image,Human activity recognition},
pages = {70--80},
publisher = {Elsevier B.V.},
title = {{Human activity recognition from 3D data: A review}},
url = {http://dx.doi.org/10.1016/j.patrec.2014.04.011},
volume = {48},
year = {2014}
}
@article{Gaglio2014,
author = {Gaglio, Salvatore and Re, Giuseppe Lo and Member, Senior and Morana, Marco},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Human Activity Recognition Process Using 3-D Posture Data.pdf:pdf},
pages = {1--12},
title = {{Human Activity Recognition Process Using 3-D Posture Data}},
year = {2014}
}
@article{Baccouche2011,
abstract = {Nurses and physicians on a stroke unit constantly face pressure and emotional stress. Physiological sensors can create awareness of one's own stress and persuade medical staff to reflect on their own behavior and coping strategies. In this study, eight nurses and physicians of a stroke unit were equipped with a wearable electrocardiography (ECG) and acceleration sensor during their everyday work in order to (a) make them aware of stress and (b) support the re-calling of experiences to identify stressors. In an interview one week later, the participants were asked to recollect stress related events through the examination of the sensor data. Although high activity levels diminished the expressiveness of the data, physicians and nurses could recall stressful events and were interested in their physiological signals. However, existing coping strategies turned out as barriers to the adoption of new tools. Future persuasive applications should focus on integration with existing coping strategies to scaffold the reflection process. © 2011 Springer-Verlag.},
author = {M\"{u}ller, Lars and Rivera-Pelayo, Ver\'{o}nica and Kunzmann, Christine and Schmidt, Andreas},
doi = {10.1007/978-3-642-25446-8},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Sequential Deep Learning for Human Action Recognition.pdf:pdf},
isbn = {978-3-642-25445-1},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Reflective learning,healthcare,physiological sensor,user study},
pages = {93--103},
title = {{Human Behavior Unterstanding}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-81855225366\&partnerID=tZOtx3y1},
volume = {7065},
year = {2011}
}
@article{Xia,
author = {Xia, Lu and Chen, Chia-chih and Aggarwal, J K},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Human Detection Using Depth Information by Kinect.pdf:pdf},
title = {{Human Detection Using Depth Information by Kinect}}
}
@article{Jiang2013,
author = {Jiang, Yun and Saxena, Ashutosh},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Infinite Latent Conditional Random Fields for Modeling Environments through Humans.pdf:pdf},
journal = {Robotics: Science and Systems},
title = {{Infinite Latent Conditional Random Fields for Modeling Environments through Humans}},
year = {2013}
}
@inproceedings{RefWorks:doc:55619fabe4b030e10feadeb9,
abstract = { The detection and recognition of generic object categories with invariance to viewpoint, illumination, and clutter requires the combination of a feature extractor and a classifier. We show that architectures such as convolutional networks are good at learning invariant features, but not always optimal for classification, while Support Vector Machines are good at producing decision surfaces from wellbehaved feature vectors, but cannot learn complicated invariances. We present a hybrid system where a convolutional network is trained to detect and recognize generic objects, and a Gaussian-kernel SVM is trained from the features learned by the convolutional network. Results are given on a large generic object recognition task with six categories (human figures, four-legged animals, airplanes, trucks, cars, and "none of the above"), with multiple instances of each object category under various poses, illuminations, and backgrounds. On the test set, which contains different object instances than the training set, an SVM alone yields a 43.3\% error rate, a convolutional net alone yields 7.2\% and an SVM on top of features produced by the convolutional net yields 5.9\%.},
author = {Huang, Fu Jie and LeCun, Yann},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2006.164},
isbn = {0769525970},
issn = {10636919},
pages = {284--291},
title = {{Large-scale learning with SVM and convolutional nets for generic object categorization}},
url = {http://seu.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwpV1bS8MwFD5MnwRhXibOC-QPdGvWNWt90-FUVKwXBj6FkzYVsc7RbuLPNyfLNvXRPbUJLbk0Tc71-wCCTsv3\_uwJIuS5MtJz1ycAcx6oLo8ijkJHoeiZK5k\_nsOLRAzOekkN5iyLxHdiQ9F0i26tZ9-UqzaqSmJRWFhBLB15BienlM0nN6cyrXBxdb2wtxDUXEC},
volume = {1},
year = {2006}
}
@article{Wang2014,
abstract = {Human action recognition is an important yet challenging task. Human actions usually involve human-object interactions, highly articulated motions, high intra-class variations and complicated temporal structures. The recently developed commodity depth sensors open up new possibilities of dealing with this problem by providing 3D depth data of the scene. This information not only facilitates a rather powerful human motion capturing technique, but also makes it possible to efficiently model human-object interactions and intra-class variations. In this paper, we propose to characterize the human actions with a novel actionlet ensemble model, which represents the interaction of a subset of human joints. The proposed model is robust to noise, invariant to translational and temporal misalignment, and capable of characterizing both the human motion and the human-object interactions. We evaluate the proposed approach on three challenging action recognition datasets captured by Kinect devices, a multiview action recognition dataset captured with Kinect device, and a dataset captured by a motion capture system. The experimental evaluations show that the proposed approach achieves superior performance to the state of the art algorithms.},
author = {Wang, Jiang and Liu, Zicheng and Wu, Ying and Yuan, Junsong},
doi = {10.1109/TPAMI.2013.198},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Learning Actionlet Ensemble for 3D Human Action Recognition.pdf:pdf},
isbn = {9781467312264},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Computer vision,Gesture,Video analysis},
number = {5},
pages = {914--927},
pmid = {24127523},
title = {{Learning actionlet ensemble for 3D human action recognition}},
volume = {36},
year = {2014}
}
@misc{Bengio2009,
abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Yoshua},
booktitle = {Foundations and Trends® in Machine Learning},
doi = {10.1561/2200000006},
eprint = {0500581},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/Greedy Layer-Wise Training of Deep Networks.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
number = {1},
pages = {1--127},
pmid = {17348934},
primaryClass = {submit},
title = {{Learning Deep Architectures for AI}},
volume = {2},
year = {2009}
}
@article{Divvala2014,
abstract = {In this paper, we introduce a fully-automated approach for learning extensive models for a wide range of variations (e.g. actions, interactions, attributes and beyond) within any concept. Our approach leverages vast resources of online books to discover the vocabulary of variance, and intertwines the data collection and modeling steps to alleviate the need for explicit human supervision in training the models. Our approach organizes the visual knowledge about a concept in a convenient and useful way, enabling a variety of applications across vision and NLP. Our online system has been queried by users to learn models for several interesting concepts including breakfast, Gandhi, beautiful, etc. To date, our system has models available for over 50,000 variations within 150 concepts, and has annotated more than 10 million images with bounding boxes.},
author = {Divvala, Santosh K and Farhadi, Ali and Guestrin, Carlos},
doi = {10.1109/CVPR.2014.412},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/Divvala\_Learning\_Everything\_about\_2014\_CVPR\_paper.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Cvpr},
pages = {3270--3277},
title = {{Learning Everything about Anything: Webly-Supervised Visual Concept Learning}},
year = {2014}
}
@article{Koppula2013a,
abstract = {Understanding human activities and object affordances are two very important skills, especially for personal robots which operate in human environments. In this work, we consider the problem of extracting a descriptive labeling of the sequence of sub-activities being performed by a human, and more importantly, of their interactions with the objects in the form of associated affordances. Given a RGB-D video, we jointly model the human activities and object affordances as a Markov random field where the nodes represent objects and sub-activities, and the edges represent the relationships between object affordances, their relations with sub-activities, and their evolution over time. We formulate the learning problem using a structural support vector machine (SSVM) approach, where labelings over various alternate temporal segmentations are considered as latent variables. We tested our method on a challenging dataset comprising 120 activity videos collected from 4 subjects, and obtained an accuracy of 79.4\% for affordance, 63.4\% for sub-activity and 75.0\% for high-level activity labeling. We then demonstrate the use of such descriptive labeling in performing assistive tasks by a PR2 robot.},
archivePrefix = {arXiv},
arxivId = {arXiv:1210.1207v2},
author = {Koppula, H. S. and Gupta, R. and Saxena, a.},
doi = {10.1177/0278364913478446},
eprint = {arXiv:1210.1207v2},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Learning Human Activities and Object Affordances from RGB-D Videos.pdf:pdf},
isbn = {0278-3649},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {3d perception,human activity detection,object affordance,personal robots,spatio-temporal context,supervised learning},
number = {8},
pages = {951--970},
title = {{Learning human activities and object affordances from RGB-D videos}},
url = {http://ijr.sagepub.com/cgi/content/long/32/8/951},
volume = {32},
year = {2013}
}
@incollection{Rumelhart1986,
abstract = {This paper presents a generalization of the perception learning procedure for learning the correct sets of connections for arbitrary networks. The rule, falled the generalized delta rule, is a simple scheme for implementing a gradient descent method for finding weights that minimize the sum squared error of the sytem's performance. The major theoretical contribution of the work is the procedure called error propagation, whereby the gradient can be determined by individual units of the network based only on locally available information. The major empirical contribution of the work is to show that the problem of local minima not serious in this application of gradient descent.},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition},
file = {:Users/boddmg/Library/Application Support/Mendeley Desktop/Downloaded/Rumelhart, Hinton, Williams - 1986 - Learning internal representations by error propagation(3).pdf:pdf},
isbn = {026268053X},
issn = {1-55860-013-2},
keywords = {Adaptive systems,Learning,Learning machines,Perceptrons,and Back propagation.,networks},
pages = {318--362},
title = {{Learning internal representations by error propagation}},
volume = {1},
year = {1986}
}
@article{Hu2014,
author = {Hu, Ninghang and Englebienne, Gwenn and Lou, Zhongyu and Kr, Ben},
doi = {10.1109/ICRA.2014.6906983},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Learning Latent Structure for Activity Recognition.pdf:pdf},
isbn = {9781479936847},
issn = {10504729},
pages = {1048--1053},
title = {{Learning Latent Structure for Activity Recognition *}},
year = {2014}
}
@article{Koppula2013b,
author = {Koppula, Hema S},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation.pdf:pdf},
journal = {Proceedings of the 30th International Conference on Machine Learning (ICML-13)},
pages = {792--800},
title = {{Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation}},
url = {http://jmlr.org/proceedings/papers/v28/koppula13.pdf},
volume = {28},
year = {2013}
}
@article{Pearson1901,
abstract = {AbstractDownload full textRelated$\backslash$n $\backslash$n $\backslash$n$\backslash$n $\backslash$n$\backslash$n$\backslash$n $\backslash$n$\backslash$n$\backslash$n$\backslash$n$\backslash$n$\backslash$n var addthis\_config = \{$\backslash$n ui\_cobrand: "Taylor \&amp; Francis Online",$\backslash$n services\_compact: "citeulike,netvibes,twitter,technorati,delicious,linkedin,facebook,stumbleupon,digg,google,more",$\backslash$n pubid: "ra-4dff56cd6bb1830b"$\backslash$n \};$\backslash$n$\backslash$n $\backslash$n$\backslash$n $\backslash$n$\backslash$n $\backslash$n $\backslash$n Add to shortlist$\backslash$n $\backslash$n $\backslash$n$\backslash$n $\backslash$n$\backslash$n $\backslash$n $\backslash$n $\backslash$n $\backslash$n Link$\backslash$n $\backslash$n$\backslash$n $\backslash$n $\backslash$n $\backslash$n Permalink$\backslash$n $\backslash$n$\backslash$n $\backslash$n $\backslash$n $\backslash$n$\backslash$n $\backslash$n$\backslash$n$\backslash$n$\backslash$n $\backslash$n $\backslash$n $\backslash$n$\backslash$n$\backslash$n$\backslash$n$\backslash$n $\backslash$n $\backslash$n http://dx.doi.org/10.1080/14786440109462720$\backslash$n $\backslash$n $\backslash$n $\backslash$n $\backslash$n $\backslash$n $\backslash$n $\backslash$n $\backslash$n $\backslash$n$\backslash$n $\backslash$n $\backslash$n $\backslash$n $\backslash$n Download Citation$\backslash$n $\backslash$n $\backslash$n$\backslash$n $\backslash$n $\backslash$n $\backslash$n Recommend to:$\backslash$n $\backslash$n $\backslash$n $\backslash$n $\backslash$n $\backslash$n$\backslash$n $\backslash$n$\backslash$n $\backslash$n $\backslash$n $\backslash$n $\backslash$n $\backslash$n$\backslash$n A friend},
author = {Pearson, Karl},
doi = {10.1080/14786440109462720},
isbn = {1941-5982},
issn = {1941-5982},
journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
pages = {559--572},
title = {{LIII. On lines and planes of closest fit to systems of points in space}},
url = {http://dx.doi.org/10.1080/14786440109462720},
volume = {2},
year = {1901}
}
@article{Ni2012,
author = {Ni, Bingbing and Moulin, Pierre and Yan, Shuicheng},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Order-Preserving Sparse Coding for Sequence Classification.pdf:pdf},
pages = {173--187},
title = {{LNCS 7573 - Order-Preserving Sparse Coding for Sequence Classification}},
year = {2012}
}
@inproceedings{Weinland2010,
abstract = {Most state-of-the-art approaches to action recognition rely on global representations either by concatenating local information in a long descriptor vector or by computing a single location independent histogram. This limits their performance in presence of occlusions and when running on multiple viewpoints. We propose a novel approach to providing robustness to both occlusions and viewpoint changes that yields significant improvements over existing techniques. At its heart is a local partitioning and hierarchical classification of the 3D Histogram of Oriented Gradients (HOG) descriptor to represent sequences of images that have been concatenated into a data volume. We achieve robustness to occlusions and viewpoint changes by combining training data from all viewpoints to train classifiers that estimate action labels independently over sets of HOG blocks. A top level classifier combines these local labels into a global action class decision.},
author = {Weinland, Daniel and \"{O}zuysal, Mustafa and Fua, Pascal},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-15558-1\_46},
isbn = {364215557X},
issn = {03029743},
number = {PART 3},
pages = {635--648},
title = {{Making action recognition robust to occlusions and viewpoint changes}},
volume = {6313 LNCS},
year = {2010}
}
@article{Jiang,
author = {Jiang, Yun and Saxena, Ashutosh},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Modeling High-Dimensional Humans for Activity Anticipation using Gaussian Process Latent CRFs.pdf:pdf},
title = {{Modeling High-Dimensional Humans for Activity Anticipation using Gaussian Process Latent CRFs}}
}
@misc{RefWorks:doc:55619e70e4b030e10feadeb5,
abstract = {The goal of this paper is to build robust human action recognition for real world surveillance videos. Local spatio-temporal features around interest points provide compact but descriptive representations for video analysis and motion recognition. Current approaches tend to extend spatial descriptions by adding a temporal component for the appearance descriptor, which only implicitly captures motion information. We propose an algorithm called MoSIFT, which detects interest points and encodes not only their local appearance but also explicitly models local motion. The idea is to detect distinctive local features through local appearance and motion. We construct MoSIFT feature descriptors in the spirit of the well-known SIFT descriptors to be robust to small deformations through grid aggregation. We also introduce a bigram model to construct a correlation between local features to capture the more global structure of actions. The method advances the state of the art result on the KTH dataset to an accuracy of 95.8\%. We also applied our approach to 100 hours of surveillance data as part of the TRECVID Event Detection task with very promising results on recognizing human actions in the real world surveillance videos. 2},
author = {Chen, Ming-yu},
title = {{MoSIFT : Recognizing Human Actions in Surveillance Videos MoSIFT : Recognizing Human Actions in Surveillance Videos}},
url = {http://seu.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwY2AwNtIz0EUrE0xSgOnCNBFYGRpbAkuHRKMk0JUEaZYGacDuR5IlaPuzc6Spe4CZm5N5AGJNDWjcvDgTNNusl5wL7TOClkkkZ-oDq3VmBmYz0E1FzEA5pErBTZCBJzw1SQE2\_i3EwJSaJ8Jg4Zsf7OkWYqUQBFmZUwWsGhTAI-UKjuAtBMUKmXkKwaVFZamg-36},
year = {2009}
}
@article{Ni2013,
abstract = {Recognizing complex human activities usually requires the detection and modeling of individual visual features and the interactions between them. Current methods only rely on the visual features extracted from 2-D images, and therefore often lead to unreliable salient visual feature detection and inaccurate modeling of the interaction context between individual features. In this paper, we show that these problems can be addressed by combining data from a conventional camera and a depth sensor (e.g., Microsoft Kinect). We propose a novel complex activity recognition and localization framework that effectively fuses information from both grayscale and depth image channels at multiple levels of the video processing pipeline. In the individual visual feature detection level, depth-based filters are applied to the detected human/object rectangles to remove false detections. In the next level of interaction modeling, 3-D spatial and temporal contexts among human subjects or objects are extracted by integrating information from both grayscale and depth images. Depth information is also utilized to distinguish different types of indoor scenes. Finally, a latent structural model is developed to integrate the information from multiple levels of video processing for an activity detection. Extensive experiments on two activity recognition benchmarks (one with depth information) and a challenging grayscale + depth human activity database that contains complex interactions between human-human, human-object, and human-surroundings demonstrate the effectiveness of the proposed multilevel grayscale + depth fusion scheme. Higher recognition and localization accuracies are obtained relative to the previous methods.},
author = {Ni, Bingbing and Pei, Yong and Moulin, Pierre and Yan, Shuicheng},
doi = {10.1109/TCYB.2013.2276433},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Multilevel Depth and Image Fusion for Human Activity Detection.pdf:pdf},
issn = {21682267},
journal = {IEEE Transactions on Cybernetics},
keywords = {Action recognition and localization,Depth sensor,Spatial and temporal context},
number = {5},
pages = {1382--1394},
pmid = {23996589},
title = {{Multilevel depth and image fusion for human activity detection}},
volume = {43},
year = {2013}
}
@book{Hutchison1973,
author = {Hutchison, David and Mitchell, John C},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/Neural.Networks.Tricks.of.the.Trade.pdf:pdf},
isbn = {9783642352881},
title = {{Neural Networks : Tricks of the Trade Second Edition}},
year = {1973}
}
@inproceedings{RefWorks:doc:55619de1e4b0e03d6aff9d14,
abstract = {This paper proposes a novel nonlinear manifold learning method for addressing the ill-posed problem of occluded human action analysis. As we know, a person can perform a broad variety of movements. To capture the multiplicity of a human action, this paper creates a low-dimensional manifold for capturing the intra-path and inter-path contexts of an event. Then, an action path matching scheme can be applied for seeking the best event path for linking the missed information between occluded persons. After that, a recovering scheme is proposed for repairing an occluded object to a complete one. Then, each action can be converted to a series of action primitives through posture analysis. Since occluded objects are handled, there will be many posture-symbol-converting errors. Instead of using a specific symbol, we code a posture using not only its best matched key posture but also its similarities among other key postures. Then, recognition of an action taken from occlude objects can be modeled as a matrix matching problem. With the matrix representation, different actions can be more robustly and effectively matched by comparing their Kullback-Leibler(KL) distances.},
author = {Chen, Li-chih and Hsieh, Jun-wei and Chuang, Chi-hung and Huang, Chang-yu and Chen, D Y},
booktitle = {International Conference on Pattern Recognition},
isbn = {9784990644116},
issn = {10514651},
number = {Icpr},
pages = {1245--1248},
publisher = {IEEE},
title = {{Occluded Human Action Analysis Using Dynamic Manifold Model}},
url = {http://seu.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwpV09T8MwED0VJhBSoRRRoJL\_QIpTx3YiNioKiIEOXZiii-0gpKhU\_fj\_nJ2EUhYGNltWpMhx7u6d794DEOMRj37ZBG0UN6nWltAD-VTlMaNKjMa49A7PZ7Ynb\_Jxpqb3etaBVmXR652EUjQ38sNws0\_z9S0W6xyrKtAK4qoRz1CJ8rdqZI\_JK\_sTrp5fdvkWwj4},
year = {2012}
}
@book{Duda2000,
abstract = {Stimulant and non-stimulant drugs can reduce symptoms of attention deficit/hyperactivity disorder (ADHD). The stimulant drug methylphenidate (MPH) and the non-stimulant drug atomoxetine (ATX) are both widely used for ADHD treatment, but their differential effects on human brain function remain unclear. We combined event-related fMRI with multivariate pattern recognition to characterize the effects of MPH and ATX in healthy volunteers performing a rewarded working memory (WM) task. The effects of MPH and ATX on WM were strongly dependent on their behavioral context. During non-rewarded trials, only MPH could be discriminated from placebo (PLC), with MPH producing a similar activation pattern to reward. During rewarded trials both drugs produced the opposite effect to reward, that is, attenuating WM networks and enhancing task-related deactivations (TRDs) in regions consistent with the default mode network (DMN). The drugs could be directly discriminated during the delay component of rewarded trials: MPH produced greater activity in WM networks and ATX produced greater activity in the DMN. Our data provide evidence that: (1) MPH and ATX have prominent effects during rewarded WM in task-activated and -deactivated networks; (2) during the delay component of rewarded trials, MPH and ATX have opposing effects on activated and deactivated networks: MPH enhances TRDs more than ATX, whereas ATX attenuates WM networks more than MPH; and (3) MPH mimics reward during encoding. Thus, interactions between drug effects and motivational state are crucial in defining the effects of MPH and ATX.},
author = {Marquand, Andre F and {De Simoni}, Sara and O'Daly, Owen G and Williams, Steven C R and Mour\~{a}o-Miranda, Janaina and Mehta, Mitul a},
booktitle = {Neuropsychopharmacology : official publication of the American College of Neuropsychopharmacology},
doi = {10.1038/npp.2011.9},
isbn = {1740-634X (Electronic)$\backslash$r0006-3223 (Linking)},
issn = {0893-133X},
number = {6},
pages = {1237--1247},
pmid = {21346736},
title = {{Pattern classification of working memory networks reveals differential effects of methylphenidate, atomoxetine, and placebo in healthy volunteers.}},
volume = {36},
year = {2011}
}
@misc{Wold1987,
abstract = {Principal component analysis of a data matrix extracts the dominant patterns in the matrix in terms of a complementary set of score and loading plots. It is the responsibility of the data analyst to formulate the scientific issue at hand in terms of \{PC\} projections, \{PLS\} regressions, etc. Ask yourself, or the investigator, why the data matrix was collected, and for what purpose the experiments and measurements were made. Specify before the analysis what kinds of patterns you would expect and what you would find exciting. The results of the analysis depend on the scaling of the matrix, which therefore must be specified. Variance scaling, where each variable is scaled to unit variance, can be recommended for general use, provided that almost constant variables are left unscaled. Combining different types of variables warrants blockscaling. In the initial analysis, look for outliers and strong groupings in the plots, indicating that the data matrix perhaps should be “polished” or whether disjoint modeling is the proper course. For plotting purposes, two or three principal components are usually sufficient, but for modeling purposes the number of significant components should be properly determined, e.g. by cross-validation. Use the resulting principal components to guide your continued investigation or chemical experimentation, not as an end in itself. },
author = {Wold, Svante and Esbensen, Kim and Geladi, Paul},
booktitle = {Chemometrics and Intelligent Laboratory Systems},
doi = {10.1016/0169-7439(87)80084-9},
isbn = {0169-7439},
issn = {01697439},
number = {1-3},
pages = {37--52},
pmid = {20931840},
title = {{Principal component analysis}},
volume = {2},
year = {1987}
}
@article{Zhang,
author = {Zhang, Chenyang and Tian, Yingli},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/RGB-D Camera-based Activity Analysis.pdf:pdf},
isbn = {1212650891},
journal = {\ldots and Conference (APSIPA ASC), 2012 Asia- \ldots},
title = {{RGB-D camera-based activity analysis}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6411826},
year = {2012}
}
@article{Zhanga,
author = {Zhang, Chenyang and Tian, Yingli},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/RGB-D Camera-based Daily Living Activity Recognition.pdf:pdf},
journal = {Journal of Computer Vision and Image Processing},
pages = {12},
title = {{RGB-D Camera-based Daily Living Activity Recognition}},
volume = {2},
year = {2012}
}
@article{Kim,
author = {Kim, Tae Hyun and Lee, Kyoung Mu},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/Kim\_Segmentation-Free\_Dynamic\_Scene\_2014\_CVPR\_paper.pdf:pdf},
title = {{Segmentation-Free Dynamic Scene Deblurring}}
}
@article{Azis2015,
author = {Azis, Nur Aziza and Choi, Ho-jin},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/3d human activity/Substitutive Skeleton Fusion for Human Action Recognition.pdf:pdf},
isbn = {9781479973033},
keywords = {computer vision,human action recognition,multi-,pose-estimation,rgb-d camera,view camera system},
title = {{Substitutive Skeleton Fusion for Human Action Recognition}},
year = {2015}
}
@article{Hinton2007,
abstract = {The uniformity of the cortical architecture and the ability of functions to move to different areas of cortex following early damage strongly suggest that there is a single basic learning algorithm for extracting underlying structure from richly structured, high-dimensional sensory data. There have been many attempts to design such an algorithm, but until recently they all suffered from serious computational weaknesses. This chapter describes several of the proposed algorithms and shows how they can be combined to produce hybrid methods that work efficiently in networks with many layers and millions of adaptive connections. © 2007 Elsevier B.V. All rights reserved.},
author = {Hinton, Geoffrey E.},
doi = {10.1016/S0079-6123(06)65034-6},
file = {:Users/boddmg/Documents/快盘/文档/技术/DL/reading\_list-To recognize shapes, first learn to generate images.pdf:pdf},
isbn = {0444528237},
issn = {00796123},
journal = {Progress in Brain Research},
keywords = {Boltzmann machines,contrastive divergence,feature discovery,generative models,learning algorithms,multilayer neural networks,shape recognition,unsupervised learning,wake-sleep algorithm},
pages = {535--547},
pmid = {17925269},
title = {{To recognize shapes, first learn to generate images}},
volume = {165},
year = {2007}
}
@article{RefWorks:doc:55619e12e4b030e10feadeb4,
abstract = {Vision-based human action recognition provides an advanced interface, and research in the field of human action recognition has been actively carried out. However, an environment from dynamic viewpoint, where we can be in any position, any direction, etc., must be considered in our living 3D space. In order to overcome the viewpoint dependency, we propose a Volume Motion Template (VMT) and Projected Motion Template (PMT). The proposed VMT method is an extension of the Motion History Image (MHI) method to 3D space. The PMT is generated by projecting the VMT into a 2D plane that is orthogonal to an optimal virtual viewpoint where the optimal virtual viewpoint is a viewpoint from which an action can be described in greatest detail, in 2D space. From the proposed method, any actions taken from different viewpoints can be recognized independent of the viewpoints. The experimental results demonstrate the accuracies and effectiveness of the proposed VMT method for view-independent human action recognition. © 2009 Elsevier B.V. All rights reserved.},
author = {Roh, Myung Cheol and Shin, Ho Keun and Lee, Seong Whan},
doi = {10.1016/j.patrec.2009.11.017},
isbn = {0167-8655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Human action recognition,Motion History Image,View-independence,Volume Motion Template},
number = {7},
pages = {639--647},
title = {{View-independent human action recognition with Volume Motion Template on single stereo camera}},
url = {http://seu.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwrV1LS8NAEB7UkyJW6ys-YA9eY5NsNo-jrVYvgmAteJCw2UygorXUFP--O5tNqVJQxFtINpBkkvlmJvN9A8CDc8\_95hNErqQksa9Iou-hEKIoNbp7hfBljoYt3XsU13dRvxs31BjbZGmRoPbwxnfbPR37bDuT0ahzT-30RLL0qDQXJCTCHWlXS8KPl925b9Z4nDR},
volume = {31},
year = {2010}
}
